{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating Siamese GNNs on Synthetic Graph Edit Distance Tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize and load model\n",
    "\n",
    "We first load a particular checkpoint of interest."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-05T09:25:08.852726Z",
     "end_time": "2024-06-05T09:25:09.073980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GraphMatchingNetwork, GraphConvolutionNetwork\n",
    "import torch\n",
    "\n",
    "# Load checkpoint (full)\n",
    "# filename = f'./checkpoints/gcn_frosty-tree-7.pth.tar'\n",
    "# filename = f'./checkpoints/gmn_glad-bird-6.pth.tar'\n",
    "filename = f'./checkpoints/gmn_rose-durian-8.pth.tar'\n",
    "checkpoint = torch.load(filename, map_location=lambda storage,\n",
    "                        loc: storage.cuda() if torch.cuda.is_available() else storage.cpu())\n",
    "\n",
    "# Retrieve state dict\n",
    "state_dict = checkpoint['state_dict']\n",
    "cfg = checkpoint['config']\n",
    "\n",
    "# Initialize model\n",
    "if cfg.model.name == 'gmn':\n",
    "    model = GraphMatchingNetwork(cfg)\n",
    "elif cfg.model.name == 'gcn':\n",
    "    model = GraphConvolutionNetwork(cfg)\n",
    "else: raise ValueError(\"Model should be either GMN or GCN\")\n",
    "\n",
    "# Load state dict\n",
    "model.load_state_dict(state_dict)  # should output \"<All keys matched successfully>\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixed Triplet Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from utils import load_pickle\n",
    "\n",
    "# Fixed dataset settings\n",
    "num_nodes = 10\n",
    "kp = 1\n",
    "kn = 2\n",
    "pe = 0.2\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_pickle(f\"./data/FixedDatasetGED_nodes={num_nodes}_kp={kp}_kn={kn}_pe={pe}.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-04T18:15:56.572065Z",
     "end_time": "2024-06-04T18:15:56.928785Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Triplet accuracy\n",
    "\n",
    "Here we compute the triplet accuracy. </br>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aae13cb925374032af1e8c2c8e54f207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance (nodes=8 kp=1 kn=2 pe=0.2):\n",
      "\tAccuracy:\t1.0\n",
      "\tLosses:\t\t0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import reshape_and_split_tensors\n",
    "from loss import triplet_loss\n",
    "from metrics import euclidean_distance\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Set model to eval model\n",
    "model.eval()\n",
    "\n",
    "for num_nodes in [8]:\n",
    "\n",
    "    # Fixed dataset settings\n",
    "    kp = 1\n",
    "    kn = 2\n",
    "    pe = 0.2\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_pickle(f\"./data/FixedDatasetGED_nodes={num_nodes}_kp={kp}_kn={kn}_pe={pe}.pickle\")\n",
    "    dataset = dataset[:100]\n",
    "\n",
    "    # Loop through data\n",
    "    rel_distances = []\n",
    "    losses = []\n",
    "    for triplet in tqdm(dataset, total=len(dataset)):\n",
    "\n",
    "        # Prepare the data\n",
    "        edge_index = triplet['edge_index']  # edge index\n",
    "        node_feats = torch.ones(triplet.num_nodes, cfg.model.node_dim)  # node features to all-ones\n",
    "        edge_feats = torch.ones(triplet.num_edges, cfg.model.edge_dim)  # edge features to all-ones\n",
    "        batch_id = triplet['order']\n",
    "\n",
    "        # Feedforward\n",
    "        _, graph_feats = model(edge_index, x1=node_feats, x2=None, edge_feats=edge_feats, batch=batch_id)\n",
    "        graph_feats = reshape_and_split_tensors(graph_feats, 4)\n",
    "\n",
    "        # Get accuracy measure\n",
    "        rel_distances.append(euclidean_distance(graph_feats[0], graph_feats[1]) \\\n",
    "                   - euclidean_distance(graph_feats[2], graph_feats[3]))\n",
    "\n",
    "        # Get loss\n",
    "        losses.append(triplet_loss(*graph_feats, cfg).detach().numpy())\n",
    "\n",
    "    # print(rel_distances)\n",
    "    losses = np.mean(losses)\n",
    "    corrects = torch.sum(torch.tensor(rel_distances) < 0)\n",
    "    accuracy = corrects / len(dataset)\n",
    "\n",
    "    print(f\"Performance (nodes={num_nodes} kp={kp} kn={kn} pe={pe}):\\n\"\n",
    "          f\"\\tAccuracy:\\t{accuracy}\\n\"\n",
    "          f\"\\tLosses:\\t\\t{losses}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-04T20:58:02.787903Z",
     "end_time": "2024-06-04T20:58:17.362778Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Looks weird ?? Why are the scores so off?**\n",
    "\n",
    "We will try it using the other dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20b9b6bbbaaf4637845a74c85c2c7efc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=20\tkp=1\tkn=2\tpe=0.2\n",
      "Performance:\n",
      "\tAcc:\t0.61160\n",
      "\tLoss:\t1.05898\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd8288c84395439b8943527a3da02cf1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=20\tkp=2\tkn=3\tpe=0.2\n",
      "Performance:\n",
      "\tAcc:\t0.61880\n",
      "\tLoss:\t1.03965\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2180a5cce373422cb70c63d1b2c0de30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=50\tkp=1\tkn=2\tpe=0.2\n",
      "Performance:\n",
      "\tAcc:\t0.87640\n",
      "\tLoss:\t0.43942\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fc23666ec6c46b8ba4f3cf91f2873bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=50\tkp=2\tkn=3\tpe=0.2\n",
      "Performance:\n",
      "\tAcc:\t0.87440\n",
      "\tLoss:\t0.45939\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae3c2a389ef34601aaab20867d42b8e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=20\tkp=1\tkn=2\tpe=0.5\n",
      "Performance:\n",
      "\tAcc:\t0.85800\n",
      "\tLoss:\t0.42660\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e60095612484728be952d91fc52900f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=20\tkp=2\tkn=3\tpe=0.5\n",
      "Performance:\n",
      "\tAcc:\t0.87480\n",
      "\tLoss:\t0.36041\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da899be8106e4decb7fd699d09dba3ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=50\tkp=1\tkn=2\tpe=0.5\n",
      "Performance:\n",
      "\tAcc:\t0.45560\n",
      "\tLoss:\t1.00548\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc1ef4ab52b94289a0c4f4af91632bc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t----- N=[20, 50]\tkp=1\tkn=2\tpe=[0.2, 0.5]\n",
      "Eval:\t----- N=50\tkp=2\tkn=3\tpe=0.5\n",
      "Performance:\n",
      "\tAcc:\t0.45680\n",
      "\tLoss:\t1.00446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data import TripletDatasetGED\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load checkpoint (full)\n",
    "# filename = f'./checkpoints/gcn_frosty-tree-7.pth.tar'\n",
    "# filename = f'./checkpoints/gmn_glad-bird-6.pth.tar'\n",
    "# filename = f'./checkpoints/gmn_rose-durian-8.pth.tar'\n",
    "filename = f'./checkpoints/gmn_decent-smoke-17.pth.tar'\n",
    "checkpoint = torch.load(filename, map_location=lambda storage,\n",
    "                        loc: storage.cuda() if torch.cuda.is_available() else storage.cpu())\n",
    "\n",
    "# Retrieve state dict\n",
    "state_dict = checkpoint['state_dict']\n",
    "cfg = checkpoint['config']\n",
    "\n",
    "# Initialize model\n",
    "if cfg.model.name == 'gmn':\n",
    "    model = GraphMatchingNetwork(cfg)\n",
    "elif cfg.model.name == 'gcn':\n",
    "    model = GraphConvolutionNetwork(cfg)\n",
    "else: raise ValueError(\"Model should be either GMN or GCN\")\n",
    "\n",
    "# Load state dict\n",
    "model.load_state_dict(state_dict)  # should output \"<All keys matched successfully>\"\n",
    "\n",
    "# Fixed dataset settings\n",
    "for num_nodes, kp, kn, pe in [[20, 1, 2, 0.2],\n",
    "                              [20, 2, 3, 0.2],\n",
    "                              [50, 1, 2, 0.2],\n",
    "                              [50, 2, 3, 0.2],\n",
    "                              [20, 1, 2, 0.5],\n",
    "                              [20, 2, 3, 0.5],\n",
    "                              [50, 1, 2, 0.5],\n",
    "                              [50, 2, 3, 0.5]]:\n",
    "\n",
    "    # Dataloader and size\n",
    "    N = 50000\n",
    "    bs = 20\n",
    "    size = int(np.ceil(N / bs))\n",
    "\n",
    "    # Initialize dataset and loader\n",
    "    dataset = TripletDatasetGED(size=size, num_nodes=num_nodes,\n",
    "                                kp=kp, kn=kn, pe=pe,\n",
    "                                permute=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Set model to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # Loop through data\n",
    "        rel_distances = []\n",
    "        losses = []\n",
    "        for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "            # Prepare the data\n",
    "            edge_index = batch['edge_index']  # edge index\n",
    "            node_feats = torch.ones(batch.num_nodes, cfg.model.node_dim)  # node features to all-ones\n",
    "            edge_feats = torch.ones(batch.num_edges, cfg.model.edge_dim)  # edge features to all-ones\n",
    "            batch_id = batch['order'] + 4 * batch['batch']\n",
    "\n",
    "            # Feedforward\n",
    "            _, graph_feats = model(edge_index, x1=node_feats, x2=None,\n",
    "                                   edge_feats=edge_feats, batch=batch_id)\n",
    "            graph_feats = reshape_and_split_tensors(graph_feats, 4)\n",
    "\n",
    "            # Compute and append (relative) distance\n",
    "            d12 = euclidean_distance(graph_feats[0], graph_feats[1])\n",
    "            d13 = euclidean_distance(graph_feats[2], graph_feats[3])\n",
    "            drel = d12 - d13  # relative distance\n",
    "            rel_distances.extend(drel.tolist())  #\n",
    "\n",
    "            # Compute and append loss\n",
    "            loss = triplet_loss(*graph_feats, cfg).numpy()\n",
    "            losses.extend(loss.tolist())\n",
    "\n",
    "        # print(rel_distances)\n",
    "        losses = np.mean(losses)\n",
    "        corrects = np.sum(np.array(rel_distances) < 0)\n",
    "        accuracy = corrects / len(dataset)\n",
    "\n",
    "        print(f\"Train:\\t----- \"\n",
    "              f\"N={cfg.data.num_nodes}\"\n",
    "              f\"\\tkp={cfg.data.kp}\"\n",
    "              f\"\\tkn={cfg.data.kn}\"\n",
    "              f\"\\tpe={cfg.data.pe}\")\n",
    "        print(f\"Eval:\\t----- \"\n",
    "              f\"N={num_nodes}\"\n",
    "              f\"\\tkp={kp}\"\n",
    "              f\"\\tkn={kn}\"\n",
    "              f\"\\tpe={pe}\")\n",
    "        print(f\"Performance:\\n\"\n",
    "              f\"\\tAcc:\\t{accuracy:.5f}\\n\"\n",
    "              f\"\\tLoss:\\t{losses:.5f}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-05T20:06:56.042945Z",
     "end_time": "2024-06-05T20:28:33.858034Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
